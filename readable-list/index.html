<!DOCTYPE html>
<html>
<head>
  <title>Tristan Havelick : A Library for Making List Webpages "Readable"</title>
  <link href="/styles.css" rel="stylesheet">
  <link href="/rss.xml" rel="alternate" title="TristanHavelick.com" type="application/rss+xml">
</head>
<body>
<div class="content">
<a href="/blog">Articles</a>
<h1>A Library for Making List Webpages "Readable"</h1>
<p>This started as a README for a potential open source project. I
decided to make a blog post to maybe get some feedback or suggestions.</p>
<h2>Status</h2>
<p>Just an idea at this point.</p>
<h2>Description</h2>
<p>There are "readability" libraries such as:</p>
<ul>
<li><a href=
"https://github.com/mozilla/readability">https://github.com/mozilla/readability</a></li>
<li><a href=
"https://github.com/buriy/python-readability">https://github.com/buriy/python-readability</a></li>
</ul>
<p>These take a webpage containing an article and strip out all the
navigation and ads. Most of these work great for pages that are
articles and some, like Mozilla's do a pretty good job of
identifying when a page is NOT an article. However, there isn't
much these libraries can currently do with pages that are lists of
articles like:</p>
<ul>
<li><a href=
"https://coloradosun.com/category/news/housing/">https://coloradosun.com/category/news/housing/</a></li>
<li><a href=
"https://drewdevault.com/">https://drewdevault.com/</a></li>
<li><a href=
"https://www.npr.org/sections/culture/">https://www.npr.org/sections/culture/</a></li>
<li><a href=
"https://www.cnn.com/health">https://www.cnn.com/health</a></li>
</ul>
<p>Here, I'm considering making a library that strips out
extraneous side navigation/ads/other junk from web pages like
these, and either returning a list of article URLs or a very simple
page with article links in a bulleted list.</p>
<p>By combining this with a standard readability algorithm/library,
one could create a simple and/or text-only view of a website than
what is typically rendered by text browsers like w3m and lynx.</p>
<p>A browser built using this could sit in a missing place in the
continuum of browser complexity:</p>
<pre><code>Offpunk &lt; THIS THING &lt; w3m/lynx/elinks &lt; visurf/netsurf/surf &lt; qutebrowser/chrome/firefox
</code></pre>
<h2>Possible Algorithm</h2>
<ol>
<li>Start with a given HTTP/S URL</li>
<li>retrieve that page</li>
<li>Parse the page and get a list of all the links on that
page</li>
<li>Remove any links that are to domains other than the one from
the original link</li>
<li>Retrieve a few, say 10,maybe random links from the remaining of
the list</li>
<li>Get a similar list of links for each of those pages</li>
<li>Get a list of the links that are common across all of the
retrieved pages. It is a reasonable assumption that these would we
be navigational links.</li>
<li>Finally, we'd return to the list of links from the original
page, and remove the links we've determined are navigational</li>
<li>At this point, we're left with links only to unique content
pages!</li>
</ol>
<h3>Flaws</h3>
<ul>
<li>If a website makes heavy use of cross linking in articles,
those articles may be unfairly excluded from the final list of
articles</li>
<li>For some sites, this would be redundant with RSS/Atom feeds a
be of lower quality</li>
<li>This probably wouldn't work (out of the box) for sites that
rely on client-side JavaScript to render content.</li>
</ul>
</div>
</body>
</html>
